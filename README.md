# LLM4REC

The ReadMe file will be edited later.

our task
our solution
data description

code description

There are two parts of the code. The first part is the modified Attention code. The second part is the progress of the proposed method.

The modified Attention code is in the folder. You can put it in the Transformers lib or create a new lib containing these codes, and name it like 'newTransformers'.

There are data preprocessing, pre-training, fine-tuning, and prediction codes in the src folder. 
First, the data preprocessing codes contain .py .py, and .py. 
